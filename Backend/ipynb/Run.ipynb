{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fffe938-5b59-46c2-9c20-a4167f5e933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8a0b6fb-686c-4b35-bf6f-17e763d89db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as VS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "305abdf8-a2ba-4b28-aa72-530159a20a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from textstat.textstat import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e6b6287-610a-4d4e-be5a-d95badbf036e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=[\"I wanna fuck you bitch!!\"]\n",
    "strr= pd.Series(s)\n",
    "type(strr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "825bfdc8-0dc6-4b3c-bc13-919de697ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Removal of punctuation and capitlization\n",
    "## 2. Tokenizing\n",
    "## 3. Removal of stopwords\n",
    "## 4. Stemming\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "#extending the stopwords to include other words used in twitter such as retweet(rt) etc.\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(tweet):  \n",
    "     # removal of extra spaces\n",
    "    regex_pat = re.compile(r'\\s+')\n",
    "    tweet_space = tweet.str.replace(regex_pat, ' ')\n",
    "\n",
    "    # removal of @name[mention]\n",
    "    regex_pat = re.compile(r'@[\\w\\-]+')\n",
    "    tweet_name = tweet_space.str.replace(regex_pat, '')\n",
    "\n",
    "    # removal of links[https://abc.com]\n",
    "    giant_url_regex =  re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "            '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    tweets = tweet_name.str.replace(giant_url_regex, '')\n",
    "    \n",
    "    # removal of punctuations and numbers\n",
    "    punc_remove = tweets.str.replace(\"[^a-zA-Z]\", \" \")\n",
    "    \n",
    "    # remove whitespace with a single space\n",
    "    newtweet=punc_remove.str.replace(r'\\s+', ' ')\n",
    "    \n",
    "    # remove leading and trailing whitespace\n",
    "    newtweet=newtweet.str.replace(r'^\\s+|\\s+?$','')\n",
    "    \n",
    "    # replace normal numbers with numbr\n",
    "    newtweet=newtweet.str.replace(r'\\d+(\\.\\d+)?','numbr')\n",
    "    \n",
    "    # removal of capitalization\n",
    "    tweet_lower = newtweet.str.lower()\n",
    "    \n",
    "    # tokenizing\n",
    "    tokenized_tweet = tweet_lower.apply(lambda x: x.split())\n",
    "    \n",
    "    # removal of stopwords\n",
    "    tokenized_tweet=  tokenized_tweet.apply(lambda x: [item for item in x if item not in stopwords])\n",
    "    \n",
    "    # stemming of the tweets\n",
    "    tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) \n",
    "    \n",
    "    for i in range(len(tokenized_tweet)):\n",
    "        tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "        tweets_p= tokenized_tweet\n",
    "    \n",
    "    return tweets_p\n",
    "\n",
    "processed_tweets = preprocess(strr)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08c5e017-d430-4f5d-8003-f4cd2b39575a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    wanna fuck bitch\n",
       "dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "359c0e53-f045-4299-97f9-200f729297a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=10000)\n",
    "tfidf = tfidf_vectorizer.fit_transform(processed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "846039dc-a480-408d-b19b-bff485d0b781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sasha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Sasha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "79c7accb-9a21-46b3-a939-96535dd1d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = VS()\n",
    "# Method to tag tags\n",
    "def count_tags(tweet_c):  \n",
    "    \n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', tweet_c)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "# Method to capture and analyse sentiment\n",
    "def sentiment_analysis(tweet):   \n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)    \n",
    "    twitter_objs = count_tags(tweet)\n",
    "    features = [sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],twitter_objs[0], twitter_objs[1],\n",
    "                twitter_objs[2]]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "# Method to store the sentimental analysis features\n",
    "def sentiment_analysis_array(tweets):\n",
    "    features=[]\n",
    "    for t in tweets:\n",
    "        features.append(sentiment_analysis(t))\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "46badc6e-1dcb-43e3-84a7-7dcbf9de3ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.798 ,  0.    ,  0.202 , -0.8353,  0.    ,  0.    ,  0.    ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_features = sentiment_analysis_array(strr)\n",
    "final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8bffea5-d545-4be3-9713-7b0028d59aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-ve</th>\n",
       "      <th>+ve</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Compound</th>\n",
       "      <th>url_tag</th>\n",
       "      <th>mention_tag</th>\n",
       "      <th>hash_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.8353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     -ve  +ve  Neutral  Compound  url_tag  mention_tag  hash_tag\n",
       "0  0.798  0.0    0.202   -0.8353      0.0          0.0       0.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the array to Dataframe\n",
    "new_features = pd.DataFrame({'-ve':final_features[:,0],'+ve':final_features[:,1],'Neutral':final_features[:,2],'Compound':final_features[:,3],\n",
    "                            'url_tag':final_features[:,4],'mention_tag':final_features[:,5],'hash_tag':final_features[:,6]})\n",
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4730b59-5488-40aa-88a5-9b29b55c8213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clubbing of tfidf and the obtained features\n",
    "tfidf_a = tfidf.toarray()\n",
    "tfidf_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5cdfdb8-8c9f-41a4-bb4a-89d34bcbcbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelling_features = np.concatenate([tfidf_a,final_features],axis=1)\n",
    "modelling_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15b037b5-cbc0-4a4a-b8e3-b76b58fd469d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc2vec_vector_0</th>\n",
       "      <th>doc2vec_vector_1</th>\n",
       "      <th>doc2vec_vector_2</th>\n",
       "      <th>doc2vec_vector_3</th>\n",
       "      <th>doc2vec_vector_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.035863</td>\n",
       "      <td>0.084266</td>\n",
       "      <td>-0.036764</td>\n",
       "      <td>-0.012341</td>\n",
       "      <td>-0.076504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc2vec_vector_0  doc2vec_vector_1  doc2vec_vector_2  doc2vec_vector_3  \\\n",
       "0         -0.035863          0.084266         -0.036764         -0.012341   \n",
       "\n",
       "   doc2vec_vector_4  \n",
       "0         -0.076504  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d827585-173f-4ac2-a3cc-e6aa0c7266fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(processed_tweets.apply(lambda x: x.split(\" \")))]\n",
    "\n",
    "# Usage of Doc2Vec\n",
    "model = Doc2Vec(documents,vector_size=5, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12af9d53-8fac-485f-9a5b-7edaa7c64447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform each document into a vector data\n",
    "doc2vec_df = processed_tweets.apply(lambda x: model.infer_vector(x.split(\" \"))).apply(pd.Series)\n",
    "doc2vec_df.columns = [\"doc2vec_vector_\" + str(x) for x in doc2vec_df.columns]\n",
    "doc2vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d51f3114-d5e9-4061-9d1d-9bf8f911f03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelling_features = np.concatenate([final_features,doc2vec_df],axis=1)\n",
    "modelling_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c324df2f-7db5-4f4d-832c-bcde8c1828e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_fin = pd.DataFrame(modelling_features)\n",
    "x_fin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c206fabb-c122-4acf-bef9-801171904fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ddc1a7a-98b6-40ea-a874-d59ddcc0b100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickled_model = pickle.load(open('model.pkl', 'rb'))\n",
    "x_fin.shape\n",
    "# pickled_model.predict(x_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49ff7eb1-f3bb-4976-9f93-83a0bbb8870d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Analysis=pickled_model.predict(x_fin)\n",
    "Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "db79ef70001eb7ca1475ddae30ac12e368d8e1929275825bc9b1812550a3fa53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
