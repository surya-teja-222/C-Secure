{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sasha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#to data preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#NLP tools\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as VS\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "#train split and fit models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#model selection\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt=\"I love you bitch asshole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword=set(stopwords.words('english'))\n",
    "stemmer = nltk. SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean (text):\n",
    "    text = str(text). lower()\n",
    "    text = re.sub('[.?]', '', text)\n",
    "    text = re.sub('https?://\\S+|www.\\S+', '', text)\n",
    "    text = re.sub('<.?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w\\d\\w', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopword]\n",
    "    text=\" \".join(text)\n",
    "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
    "    text=\" \".join(text)\n",
    "    return text\n",
    "cleaned_cmt=clean(cmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_analyzer = VS()\n",
    "\n",
    "# def count_tags(tweet_c):  \n",
    "    \n",
    "#     space_pattern = '\\s+'\n",
    "#     giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "#         '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "#     mention_regex = '@[\\w\\-]+'\n",
    "#     hashtag_regex = '#[\\w\\-]+'\n",
    "#     parsed_text = re.sub(space_pattern, ' ', tweet_c)\n",
    "#     parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "#     parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "#     parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "#     return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "# def sentiment_analysis(tweet):   \n",
    "#     sentiment = sentiment_analyzer.polarity_scores(tweet)    \n",
    "#     twitter_objs = count_tags(tweet)\n",
    "#     features = [sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],twitter_objs[0], twitter_objs[1],\n",
    "#                 twitter_objs[2]]\n",
    "#     return features\n",
    "\n",
    "# def sentiment_analysis_array(tweets):\n",
    "#     features=[]\n",
    "#     for t in tweets:\n",
    "#         features.append(sentiment_analysis(t))\n",
    "#     return np.array(features)\n",
    "\n",
    "# sentiment = sentiment_analysis_array(cmt)\n",
    "# sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment = pd.DataFrame({'-ve':sentiment[:,0],'+ve':sentiment[:,1],'Neutral':sentiment[:,2],'Compound':sentiment[:,3],\n",
    "#                             'url_tag':sentiment[:,4],'mention_tag':sentiment[:,5],'hash_tag':sentiment[:,6]})\n",
    "# Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickled_model = pickle.load(open('model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m cv\u001b[38;5;241m=\u001b[39mCountVectorizer()\n\u001b[1;32m----> 2\u001b[0m inp \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mtransform(cmt)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m      3\u001b[0m output\u001b[38;5;241m=\u001b[39mpickled_model\u001b[38;5;241m.\u001b[39mpredict(cmt)\n",
      "File \u001b[1;32mc:\\Users\\Sasha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1381\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1365\u001b[0m \u001b[39m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[0;32m   1366\u001b[0m \n\u001b[0;32m   1367\u001b[0m \u001b[39mExtract token counts out of raw text documents using the vocabulary\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1378\u001b[0m \u001b[39m    Document-term matrix.\u001b[39;00m\n\u001b[0;32m   1379\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(raw_documents, \u001b[39mstr\u001b[39m):\n\u001b[1;32m-> 1381\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1382\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIterable over raw text documents expected, string object received.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m     )\n\u001b[0;32m   1384\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1386\u001b[0m \u001b[39m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "cv=CountVectorizer()\n",
    "inp = cv.transform(cmt).toarray()\n",
    "output=pickled_model.predict(cmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db79ef70001eb7ca1475ddae30ac12e368d8e1929275825bc9b1812550a3fa53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
